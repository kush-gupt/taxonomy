version: 3
domain: Scheduling
created_by: kush-gupt
document_outline: |
  Overview of the Flux scheduler, explaining what it is, how
  they came to be, and the relationship with High Performance
  Computing and cloud native technologies.

document:
  repo: https://github.com/kush-gupt/taxonomy-knowledge-docs.git
  commit: 0ca22206147a71ddf852df4c745176e3a2fe23bf
  patterns:
    - '*.pdf'

# Flux 2014 Starter ----------------------------------------------------------------------------------------------------
seed_examples:
  - context: |-
      An HPC batch system must generate and provide a complete set of statistics for
      the jobs that are run and the resources that are consumed.  This begins with a
      list of job records, both individual and aggregated.  Users and managers will
      want to look back over time to see what jobs ran, what resources they ran on,
      and whether they ran successfully to completion.  Managers also require reports
      of how computing resource investments are utilized and by which users, groups
      and projects.  Fair-share calculations are based off the difference between what
      resources were promised and which resources were ultimately used.
    questions_and_answers:
      - question: What is one of the responsibilities of an HPC batch system?
        answer: |-
          An HPC batch system is responsible for generating and providing a complete set
          of statistics for the jobs that are run and the resources that are consumed.
      - question: |-
          What kind of information can be obtained from the job records provided by an HPC
          batch system?
        answer: |-
          The job records provided by an HPC batch system include information about the
          jobs that have run, the resources they ran on, and whether they ran successfully
          to completion.
      - question: |-
          Given the information about fair-share calculations, how are they typically
          determined?
        answer: |-
          Fair-share calculations are based off the difference between what resources were
          promised and which resources were ultimately used.
  - context: |-
      This primer introduces the concepts of High Performance Computing, scheduling,
      and resource management.  It focuses first on the state of the art and then
      introduces the issues under consideration in the design of a next generation
      resource manager known as Flux.  The Flux design addresses not just the
      challenges of resource management at a much larger scale of systems, but it also
      enables and promotes a much broader and richer model that invites collaboration
      with areas whose requirements cannot be met in current systems.
    questions_and_answers:
      - question: What are the main aspects addressed by the design of the Flux resource
          manager?
        answer: |-
          The design of the Flux resource manager addresses the challenges of resource
          management at a larger scale of systems and enables and promotes collaboration
          with areas whose requirements cannot be met in current systems.
      - question: Based on the information provided, why was the Flux resource manager
          designed?
        answer: |-
          The Flux resource manager was designed to address the challenges of resource
          management at a larger scale of systems and to enable and promote collaboration
          with areas whose requirements cannot be met in current systems.
      - question: How does Flux differ from traditional resource management approaches?
        answer: |-
          Flux differs from traditional resource management approaches by  providing a
          hierarchical, fully-nested framework where each job is a complete instance
          of the framework, enabling more flexible resource allocation and customizable
          scheduling policies at different levels.

  - context: |-
      The term High performance computing (HPC) as it is commonly used refers to a
      category of computing systems that combine computing power from multiple units
      to deliver vastly higher performance than desktop computers can provide.  HPC is
      distinguished not just by computing power.  HPC systems are designed to run a
      variety of large applications for extended periods of time in order to solve
      complex problems.  HPC is distinguished from web servers in that they dedicate
      requested resources to run applications provided by users.  Current HPC systems
      are distinguished from cloud services like AWS in that they generally provide a
      single operating system with a runtime environment managed by the facility.
    questions_and_answers:
      - question: What is commonly understood by the term High performance computing
          (HPC)?
        answer: |-
          High performance computing (HPC) refers to a category of computing systems that
          combine computing power from multiple units to deliver vastly higher performance
          than desktop computers.
      - question: How does HPC differ from web servers and cloud services like AWS?
        answer: |-
          HPC systems differ from web servers in that they dedicate requested resources to
          run applications provided by users. They differ from cloud services like AWS in
          that they generally provide a single operating system with a runtime environment
          managed by the facility.
      - question: |-
          Given the description, why are HPC systems more suitable for solving complex
          problems over extended periods compared to desktop computers or cloud services?
        answer: |-
          HPC systems are designed to run a variety of large applications for extended
          periods of time in order to solve complex problems, making them more suitable
          for such tasks compared to desktop computers or cloud services.
# Flux 2018 ----------------------------------------------------------------------------------------------------
  - context: "Our evaluation with three recent workflow efforts at LLNL shows that\
      \ Flux\nsignificantly overcomes all of the stated challenges. Our performance\n\
      measurements on synthetic and real ensemble-based workflows suggest that our\n\
      hierarchical scheduling approach can improve the job throughput performance\
      \ of\nthese workflows by 48 \xD7 . Further, our case study on the Cancer Moonshot\
      \ Pilot2\nproject shows that Flux can efficiently co-schedule a new workflow\
      \ that employs\nmachine learning to couple a large continuum model-based application\
      \ with an\nensemble of thousands of MD simulations starting and stopping during\
      \ a run at\nhigh speed. Finally, our integration with Merlin, a workflow management\
      \ system\ndesigned to support next-generation machine learning on HPC, shows\
      \ that Flux\nsignificantly enables not only co-scheduling of various task types\
      \ within each\nensemble but also its needs for high portability and task communication\
      \ and\ncoordination."
    questions_and_answers:
      - question: |-
          How many times can Flux improve the job throughput performance of workflows
          according to the performance measurements?
        answer: "48"
      - question: What are the main benefits of using Flux in the Cancer Moonshot
          Pilot2 project?
        answer: |-
          Flux can efficiently co-schedule a new workflow that employs machine learning to
          couple a large continuum model-based application with an ensemble of thousands
          of MD simulations starting and stopping during a run at high speed.
      - question: |-
          Based on the integration with Merlin, what additional capabilities does Flux
          enable for task communication and coordination?
        answer: |-
          Flux enables high portability and efficient co-scheduling of various task types
          within each ensemble, enhancing task communication and coordination.
  - context: |-
      System-level solutions can be broken down into centralized, limited
      hierarchical, and decentralized schedulers. Centralized schedulers use a single,
      global scheduler that maintains and tracks the full knowledge of jobs and
      resources to make scheduling decisions. This scheduling model is simple and
      effective for moderate-size clusters, making it the state of the practice in
      most cloud and HPC centers today. Cloud schedulers such as Swarm [18] and
      Kubernetes [19] and HPC schedulers such as SLURM [8], MOAB [10], LSF [9], and
      PBSPro [11] are centralized. While simple, these centralized schedulers are
      capped at tens of jobs/sec [25], provide limited to no support for co-scheduling
      of heterogeneous tasks [26], have limited APIs, and cannot be easily nested
      within other system schedulers.
    questions_and_answers:
      - question: What is the scheduling model used by centralized schedulers?
        answer: |-
          Centralized schedulers use a single, global scheduler that maintains and tracks
          the full knowledge of jobs and resources to make scheduling decisions.
      - question: |-
          Given the limitations of centralized schedulers, what type of scheduling model
          might be more suitable for handling a large number of jobs and providing support
          for co-scheduling of heterogeneous tasks?
        answer: |-
          A decentralized scheduling model might be more suitable as it can handle a large
          number of jobs and provide support for co-scheduling of heterogeneous tasks.
      - question: What are some limitations of centralized schedulers like SLURM, MOAB, LSF, and PBSPro?
        answer: |-
          Centralized schedulers are capped at tens of jobs/second, provide limited to no
          support for co-scheduling of heterogeneous tasks, have limited APIs, and cannot
          be easily nested within other system schedulers.
  - context: |-
      Flux's user-driven, customizable approach to scheduling provides inherent
      support for co-scheduling. Flux's flexible design allows users to decide whether
      or not co-scheduling should be configured and also lets users choose their own
      scheduling policies within the scope of an instance. With the help of the job
      submission API, several tasks can efficiently coexist on a single node without
      any restrictions on their number, type, or resource requirements. This allows
      for submission and tuning at all possible levels of heterogeneity within a node
      (and across nodes), including individual cores, a set of cores, sockets, GPUs,
      or burst buffers.
    questions_and_answers:
      - question: Does Flux support co-scheduling?
        answer: Yes, Flux supports co-scheduling.
      - question: What are the key features of Flux's approach to scheduling and co-scheduling?
        answer: |-
          Flux's scheduling approach is user-driven and customizable, providing inherent
          support for co-scheduling. It allows users to decide whether to configure co-
          scheduling and choose their own scheduling policies. The job submission API
          enables multiple tasks to coexist on a single node, regardless of their number,
          type, or resource requirements, allowing for submission and tuning at various
          levels of heterogeneity.
      - question: |-
          Given Flux's support for co-scheduling and its flexible design, why might users
          choose to use Flux for scheduling tasks on their nodes?
        answer: |-
          Users might choose to use Flux for scheduling tasks because it allows for
          efficient coexistence of several tasks on a single node without restrictions on
          their number, type, or resource requirements. This flexibility enables
          submission and tuning at all possible levels of heterogeneity within a node and
          across nodes, including individual cores, sets of cores, sockets, GPUs, or burst
          buffers.
# Fluxion 2023 ----------------------------------------------------------------------------------------------------
  - context: |-
      We presented a novel graph-based resource model and its implementation, Fluxion,
      for HPC and converged computing scheduling. Our model addresses the limitations
      of existing node-centric models, and supports converged computing environments,
      disaggregated systems, and complex and elastic modern workflows. We described
      the novel architecture, procedures for improving scalability, and algorithms
      comprising Fluxion and discussed how these contributions allow Fluxion to
      schedule resources efficiently. We discussed several emerging use cases which
      our model enables and evaluated its scalability. Future work involves optimizing
      our model's implementation and applying it to scheduling environments with
      dynamic resources and hierarchies. The powerful combination of a directed-graph
      resource model with fully-hierarchical scheduling and architectural separation
      of concerns allows Fluxion to schedule virtually any resource types, including
      types not yet devised.
    questions_and_answers:
      - question: |-
          What are the main features of the Fluxion model that make it suitable for
          converged computing environments, disaggregated systems, and complex and elastic
          modern workflows?
        answer: |-
          The Fluxion model is a directed-graph resource model with fully-hierarchical
          scheduling and architectural separation of concerns, which allows it to schedule
          virtually any resource types, including those not yet devised.
      - question: |-
          Given the features and capabilities of the Fluxion model, why is it well-suited
          for future work involving scheduling environments with dynamic resources and
          hierarchies?
        answer: |-
          The Fluxion model's features, such as its directed-graph resource model, fully-
          hierarchical scheduling, and architectural separation of concerns, make it
          adaptable and flexible, allowing it to handle dynamic resources and hierarchies
          effectively.
      - question: What is Fluxion and how does it relate to Flux?
        answer: |-
          Fluxion is Flux's graph-based scheduler component that implements a novel
          resource model for HPC and converged computing scheduling. It was formerly
          known as flux-sched and provides advanced batch scheduling capabilities to
          the Flux framework.
  - context: |-
      In this changing landscape, HPC resource managers and job schedulers relying on
      restrictive approaches to solve emerging problems create a technology gap. One
      such gap is in the design of resource models , which define how information
      about resource types and job requests is stored internally. Existing HPC
      schedulers use a node-centric or core-centric design. While such a design is
      ideal for traditional applications and is efficient in terms of scheduler
      performance, it falls short in supporting emerging HPC workloads. For example,
      it does not offer natural ways to express resource relationships. As a result,
      it cannot be used to represent complex resource constraints or to capture
      elasticity that is inherent to modern workloads. It also cannot be extended
      easily to represent extremely heterogeneous resources or flow resources such as
      power or network bandwidth. Such a design also limits expressibility for
      seamless scheduling across disaggregated resources or converged HPC and Cloud
      platforms [9, 28, 30].
    questions_and_answers:
      - question: What kind of design do existing HPC schedulers use?
        answer: Existing HPC schedulers use a node-centric or core-centric design.
      - question: |-
          What are the limitations of the existing design of HPC schedulers in supporting
          emerging HPC workloads?
        answer: |-
          The existing design of HPC schedulers does not offer natural ways to express
          resource relationships, cannot be used to represent complex resource constraints
          or to capture elasticity that is inherent to modern workloads, and cannot be
          extended easily to represent extremely heterogeneous resources or flow resources
          such as power or network bandwidth. It also limits expressibility for seamless
          scheduling across disaggregated resources or converged HPC and Cloud platforms.
      - question: |-
          Given the limitations of the existing node-centric or core-centric design of HPC
          schedulers, what kind of design might be more suitable for supporting emerging
          HPC workloads?
        answer: |-
          A design that offers natural ways to express resource relationships, can
          represent complex resource constraints and capture elasticity, can be extended
          to represent heterogeneous resources and flow resources, and can facilitate
          seamless scheduling across disaggregated resources or converged HPC and Cloud
          platforms would be more suitable for supporting emerging HPC workloads.
  - context: |-
      Our resource model must also provide efficient time management : it must keep
      track of the state changes of resources over time in support of various queuing,
      reservation, and backfilling policies [15]. The model accomplishes this by
      directly integrating a highly efficient resource-time state tracking and search
      mechanism into each vertex, called Planner , (Step (6), Section 4.1). Even after
      we judiciously choose the right levels of detail for our resource graph store,
      the matching process for allocations requires further optimization. Thus,
      Fluxion utilizes pruning filters for scalability, as shown at Step (6). Pruning
      filters are installed at higher-level resource vertices (such as compute-rack-
      level vertices) to keep track of the aggregate amounts of available lower-level
      resources (e.g., individual cores). Fluxion also introduces a novel Scheduler-
      Driven Filter Update (SDFU) algorithm (Step (5)) to update these filters.
      Finally, once Fluxion determines the best matching resource subgraph, it is
      emitted as a selected resource set at (7), which can then be allocated to the
      user for their application by the underlying RM. The RM can then make use of
      this resource set to contain, bind and execute the target program(s) within
      those resources.
    questions_and_answers:
      - question: |-
          What is the name of the mechanism integrated into each vertex for efficient time
          management?
        answer: Planner
      - question: What are the three methods Fluxion uses to optimize resource allocation?
        answer: |-
          Fluxion uses pruning filters for scalability, a Scheduler-Driven Filter Update
          (SDFU) algorithm to update these filters, and it emits the best matching
          resource subgraph as a selected resource set for allocation.
      - question: Given the information provided, why does Fluxion introduce the SDFU
          algorithm?
        answer: |-
          Fluxion introduces the SDFU algorithm to update the pruning filters that keep
          track of the aggregate amounts of available lower-level resources.
# Flux FAQ ----------------------------------------------------------------------------------------------------
  - context: |-
      Flux is a new resource manager and job scheduler for high-performance computing
      (HPC) systems. It is designed to be flexible, scalable, and efficient, addressing
      the challenges of modern HPC workloads and enabling new use cases.
    questions_and_answers:
      - question: What is Flux?
        answer: |-
          Flux is a new resource manager and job scheduler for high-performance computing
          (HPC) systems. It is designed to be flexible, scalable, and efficient, addressing
          the challenges of modern HPC workloads and enabling new use cases.
      - question: What are the main goals of Flux?
        answer: |-
          The main goals of Flux are to provide a flexible and scalable resource management
          solution for HPC systems, address the challenges of modern HPC workloads, and
          enable new use cases.
      - question: |-
          What is Flux's approach to resource allocation?
        answer: |-
          Flux's approach to resource allocation uses hierarchical, multi-level management
          and scheduling schemes with a graph-based resource model. This enables more
          sophisticated scheduling of resources beyond the traditional node-centric view,
          including consideration of networks, I/O, and power interconnections.
# Flux Operator ----------------------------------------------------------------------------------------------------
  - context: |-
      This paper introduces the Flux Operator, 19 the next step in work to explore
      integration of a traditional HPC scheduler within Kubernetes. The Flux Operator
      is a Kubernetes operator 20 that handles all the setup and configuration of a
      fully fledged Flux cluster inside of Kubernetes itself, allowing the user to
      efficiently bring up and down an entire HPC cluster for a scoped set of work
      that optimizes for important aspects of HPC. This paper first reviews the design
      and architecture of the Flux Operator (Section 2), discussing Kubernetes
      abstractions for efficient networking and node setup. We then discuss how these
      design decisions impact essential needs such as workflows that use message
      passing interfaces (MPI), and experimental features like scaling and elasticity
      (Section 4). Finally, experimental work demonstrates the Flux Operator enable
      superior performance for a well-known HPC application over the MPI Operator, the
      primary available option at the time for running MPI workflows (Section 3). The
      paper concludes with discussion for anticipated future work, considerations for
      workflow design, and vision for the future (Section 5).
    questions_and_answers:
      - question: What is the Flux Operator?
        answer: |-
          The Flux Operator is a Kubernetes operator that handles the setup and
          configuration of a fully fledged Flux cluster inside of Kubernetes itself,
          allowing the user to efficiently bring up and down an entire HPC cluster for a
          scoped set of work that optimizes for important aspects of HPC.
      - question: What are the main sections discussed in the paper?
        answer: |-
          The paper discusses the design and architecture of the Flux Operator in Section
          2, the impact of design decisions on workflows like MPI and experimental
          features in Section 4, and presents experimental work in Section 3. The paper
          also concludes with discussion for future work, considerations for workflow
          design, and vision for the future in Section 5.
      - question: |-
          Based on the information provided, how does the Flux Operator enhance
          performance compared to the MPI Operator?
        answer: |-
          The Flux Operator enables superior performance for a well-known HPC application
          over the MPI Operator, the primary available option at the time for running MPI
          workflows.
  - context: |-
      The popularity and economic clout behind cloud computing presents a challenge
      for the high performance computing community -to resist new paradigms of cloud-
      native technologies and fall behind, losing talent and customers, or to embrace
      it, pursuing technological innovation, and viewing the shift as an opportunity
      for collaboration and growth. The latter approach, a movement identified as '
      converged computing ' is a good path to take, and this work represents an early
      start towards that desired future. The work here starts with one of the highest
      levels for running workflows -the orchestration tool -and has thought about the
      convergence of the HPC workload manager Flux Framework with the cloud
      orchestration framework Kubernetes. The Flux Operator is an example of
      convergence of technologies in this workload management space, and has
      demonstrated superior performance to the current equivalent in the space. In
      sharing this process of thinking about design to implementation, the authors of
      this paper hope to start discussion with the larger community that spans cloud
      and HPC for how to think about working on convergence for other types of
      software and technology. This work is a shining example of the collaboration and
      fun possible. The sharing of these results at Kubecon Amsterdam 23, ' 23
      inspired collaboration, discussion, and excitement about the converged computing
      movement. Projects and work are underway to address gaps that have been
      identified, and each a collaboration between computer scientists and cloud
      developers. The authors of this paper hope that this early work inspires, and
      allows for continued discussion for innovation in this space for not just
      workloads and scheduling, but also the challenges around it -storage, tenancy,
      and cost-estimation, among others. Through collaboration and creativity of
      design, pathways can be discovered for moving seamlessly between the spaces of
      cloud and HPC. Converged computing is a paradigm shift, and it s up to the HPC
      and ' cloud communtities to decide to embrace change and grow from it, or fight
      it off. This work chooses the first approach, and embraces it with hopes for a
      better future, and stronger more reproducible science.
    questions_and_answers:
      - question: |-
          What fundamental challenge does cloud computing present to the high performance
          computing (HPC)community?
        answer: |-
          Cloud computing presents the HPC community with a fundamental dilemma: either
          resist new cloud-native paradigms and risk falling behind (losing talent and
          scientists), or embrace these technologies as opportunities for innovation and
          growth. The Flux Operator paper strongly advocates for the latter approach, termed "converged
          computing," which seeks to integrate the strengths of both HPC and cloud
          technologies. The Flux Operator project represents an early implementation of
          this philosophy by integrating the Flux Framework (an HPC workload manager)
          with Kubernetes (a cloud orchestration framework), demonstrating superior
          performance compared to traditional approaches.
      - question: |-
          What specific technical and organizational challenges must the converged
          computing movement address to succeed, and why are these challenges significant?
        answer: |-
          The converged computing movement must faces multiple interconnected challenges.
          These include: 1) Technical challenges such as storage, multi-tenancy,
          and cost estimation, which are critical for seamless integration between
          cloud and HPC environments; 2) Organizational challenges of building
          collaborative teams spanning traditionally separate HPC and cloud communities;
          and 3) Cultural challenges in reconciling the performance-first
          mindset of HPC with the flexibility-oriented approach of cloud computing.
          These challenges are significant because addressing them requires fundamental
          rethinking of both technologies and organizational structures rather than
          iterative adaptations.
      - question: |-
          Why is converged computing a paradigm shift, and
          what does this imply for the future of scientific computing?
        answer: |-
          Converged computing is a paradigm shift because it fundamentally
          reimagines the relationship between HPC and cloud computing - not
          as competing approaches, but as complementary technologies that can be
          integrated to create systems greater than the sum of their parts. This shift
          implies a transformative future for scientific computing where researchers
          benefit simultaneously from the performance characteristics of HPC and the
          flexibility, accessibility, and resource efficiency of cloud computing. The
          authors envision this convergence leading to "stronger more reproducible
          science" by combining the computational power needed for complex simulations
          with the containerization, orchestration, and collaboration tools that have
          revolutionized software development in cloud environments.
  - context: |-
      Running a single MiniCluster to create an isolated Flux cluster in Kubernetes is
      an initial step, but it is insufficient for realworld use cases of complex
      workflows. While it would be possible to shell into or otherwise interact with
      the cluster and run a workflow tool that implements Flux as an executor, 34 -36
      this also does not enable features needed for complex, heterogeneous workflows
      that might require different sizes or configurations of MiniClusters. We are
      considering integration of the MiniCluster custom resource definition as a
      first-class citizen into workflow tools in future work.
    questions_and_answers:
      - question: |-
          What is the current insufficiency of running a single MiniCluster for real-world
          use cases?
        answer: |-
          The current MiniCluster setup is insufficient for real-world use cases of
          complex workflows due to its isolation and lack of support for features needed
          for heterogeneous workflows with different sizes or configurations of
          MiniClusters.
      - question: |-
          What are the potential improvements being considered for MiniCluster integration
          in workflow tools?
        answer: |-
          The potential improvements being considered for MiniCluster integration in
          workflow tools include integrating the MiniCluster custom resource definition as
          a first-class citizen, which would enable features needed for complex,
          heterogeneous workflows with different sizes or configurations of MiniClusters.
      - question: |-
          Given the current limitations of a single MiniCluster, why would integrating the
          MiniCluster custom resource definition as a first-class citizen into workflow
          tools be a beneficial approach for real-world use cases?
        answer: |-
          Integrating the MiniCluster custom resource definition as a first-class citizen
          into workflow tools would enable features needed for complex, heterogeneous
          workflows with different sizes or configurations of MiniClusters, addressing the
          current limitations of a single MiniCluster.
  - context: |-
      To complete the early work in autoscaling, we considered the concept of
      bursting, 33 which means not just extending the size of a local cluster, but
      actually extending work to external resources. The bursting work for Flux would
      extend this approach to not just deploy external resources, but allow the lead
      broker to connect to brokers that are deployed in the other clusters. As an
      example, a Kubernetes cluster running on Google Cloud might burst to a cluster
      running on Compute Engine (CE), or to a cluster on Amazon Elastic Kubernetes
      Service (EKS).
    questions_and_answers:
      - question: |-
          What are the two examples of external resources that a Kubernetes cluster
          running on Google Cloud might burst to?
        answer: |-
          The two examples of external resources that a Kubernetes cluster running on
          Google Cloud might burst to are a cluster running on Compute Engine (CE), or to
          a cluster on Amazon Elastic Kubernetes Service (EKS).
      - question: |-
          What are the key benefits of implementing bursting capability in Flux for HPC workflows?
        answer: |-
          Key benefits of implementing bursting capability in Flux include improved resource
          utilization, cost optimization through using external resources only when needed,
          handling workload spikes without maintaining permanent infrastructure, and enabling
          workload portability across different cloud providers and on-premises resources.
      - question: |-
          What technical challenges must be addressed for effective implementation of bursting in Flux?
        answer: |-
          Effective implementation of bursting in Flux requires addressing challenges such as
          maintaining secure network connectivity between clusters, handling network latency
          between distributed resources, managing consistent data access across environments,
          ensuring broker synchronization, and accommodating different resource policies and
          constraints across cluster boundaries.
  - context: |-
      The first requirement for using the Flux Operator is access to a Kubernetes
      cluster with sufficient permission to deploy each of the abstractions discussed
      in Section 2, including Deployment, Service, Job, and ConfigMap. The Flux
      Operator pre-built container images are provided for nodes with amd64 or arm64
      architectures. Operation comes down to deploying these objects with the kubectl
      command line tool and then using the same tool to deploy the MiniCluster CRD.
      Upon creation, the operator reconciles in a loop until the state of objects in
      Kubernetes matches the desired state specified in the CRD. This desired state
      encompasses the creation of the MiniCluster, which includes an indexed Job with
      a group of pods for the cluster, each containing a flux broker and volumes for
      configuration. A Headless Service networks the pods together to complete the
      MiniCluster. The specific attributes of the cluster (e.g., size, application
      image, command) can be customized in this YAML file, resulting in a Flux
      MiniCluster that exists to run a command and clean up (akin to a batch job), or
      an interactive cluster that can address several use cases and interactions
      discussed in Section 4. The final Flux MiniCluster is illustrated in Figure 1.
    questions_and_answers:
      - question: |-
          What is the name of the command line tool used to deploy objects with the Flux
          Operator, and what is its role in the deployment process?
        answer: |-
          The command line tool used to deploy objects with the Flux Operator is kubectl,
          the standard Kubernetes command-line interface. In the Flux Operator deployment
          process, kubectl serves two critical functions: first, it's used to deploy the
          core Kubernetes objects (Deployments, Services, Jobs, and ConfigMaps) required
          by the operator; second, it's used to deploy the MiniCluster Custom Resource
          Definition (CRD) itself, which triggers the operator's reconciliation loop to
          create the desired Flux environment.
      - question: |-
          What are the main Kubernetes objects that the Flux Operator deploys, and how do
          they work together?
        answer: |-
          The Flux Operator deploys an integrated set of Kubernetes objects that work
          together to create a functioning HPC environment: 1) The MiniCluster Custom
          Resource Definition (CRD) which defines the desired state; 2) An indexed Job
          that creates a scalable set of pods forming the compute cluster; 3) Pod
          containers, each running a Flux broker with associated configuration volumes for
          state management; 4) A Headless Service that provides the critical network
          mesh connecting all brokers; and 5) Supporting ConfigMaps and optional Secrets
          for configuration data. This architecture enables Flux's hierarchical scheduling
          capabilities while leveraging Kubernetes' orchestration features.
      - question: |-
          what are the primary use cases and advantages of deploying
          a Flux MiniCluster in Kubernetes?
        answer: |-
          The Flux MiniCluster offers two operational modes with distinct advantages:
          1) Ephemeral batch mode, where it efficiently executes a specific HPC workload
          or command and automatically cleans up resources afterward, ideal for pipeline
          integration; and 2) Interactive cluster mode, which provides a persistent HPC
          environment within Kubernetes for iterative scientific workflows, development,
          or complex multi-stage computations. Key advantages include the customizability
          of cluster attributes (size, application image, commands) through simple YAML
          configuration, superior performance for HPC workloads compared to alternatives
          like the MPI Operator, and the convergence of cloud-native orchestration with
          HPC-specific resource management capabilities.
